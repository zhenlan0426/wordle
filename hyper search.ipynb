{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e9a2d44c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functions_pt import *\n",
    "import pickle\n",
    "from torch.optim import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c359764b",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "lr = 1e-4\n",
    "clip = 1\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45745666",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b576db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "ys = np.load('ys.npy')\n",
    "weights = np.load('weights.npy')\n",
    "length = np.load('length.npy')\n",
    "words_embed = np.load('possible_words_embed.npy')\n",
    "with open('indexes.pkl', 'rb') as f:\n",
    "    indexes = pickle.load( f)\n",
    "\n",
    "# train/val split\n",
    "np.random.seed(0)\n",
    "train_idx = np.random.rand(ys.shape[0])>0.2\n",
    "val_idx = np.logical_not(train_idx)\n",
    "\n",
    "ys_train, ys_val = ys[train_idx], ys[val_idx]\n",
    "weights_train, weights_val = weights[train_idx], weights[val_idx]\n",
    "length_train, length_val = length[train_idx], length[val_idx]\n",
    "indexes_train = [idx for idx,bol in zip(indexes,train_idx) if bol]\n",
    "indexes_val = [idx for idx,bol in zip(indexes,train_idx) if not bol]\n",
    "\n",
    "# set up dataloader\n",
    "dataset_train = CustomDataset(ys_train,length_train,indexes_train,words_embed)\n",
    "sampler = WeightedRandomSampler(weights_train, 50000)\n",
    "dataset_train = DataLoader(dataset_train, batch_size=batch_size, sampler=sampler,collate_fn=collate)\n",
    "\n",
    "dataset_val = CustomDataset(ys_val,length_val,indexes_val,words_embed)\n",
    "sampler = WeightedRandomSampler(weights_val, 10000)\n",
    "dataset_val = DataLoader(dataset_val, batch_size=batch_size, sampler=sampler,collate_fn=collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e6d1780",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a580a586",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size:64, mul_factor:1, layers:4, agg:max, dropout:0.32450477542810136, MSE:21.977670673280954\n",
      "\n",
      "size:160, mul_factor:2, layers:4, agg:sum, dropout:0.00825110002164419, MSE:3.170150564983487\n",
      "\n",
      "size:64, mul_factor:2, layers:3, agg:sum, dropout:0.28283533250553783, MSE:3.8977135643363\n",
      "\n",
      "size:128, mul_factor:2, layers:4, agg:mean, dropout:0.30103819777023416, MSE:9.27444832213223\n",
      "\n",
      "size:160, mul_factor:2, layers:4, agg:sum, dropout:0.08712807594237947, MSE:3.6702809017151594\n",
      "\n",
      "size:128, mul_factor:1, layers:3, agg:max, dropout:0.29262215212133097, MSE:12.708432290703058\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [4]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m model \u001b[38;5;241m=\u001b[39m pointNet(layers,embed_size,agg,dropout,multiple_factor)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      8\u001b[0m opt \u001b[38;5;241m=\u001b[39m Adam([    {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m'\u001b[39m: [p \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mparameters() \u001b[38;5;28;01mif\u001b[39;00m p \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m model\u001b[38;5;241m.\u001b[39mw]},\\\n\u001b[1;32m      9\u001b[0m                 {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m'\u001b[39m: model\u001b[38;5;241m.\u001b[39mw, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m'\u001b[39m: lr\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m4\u001b[39m}\n\u001b[1;32m     10\u001b[0m             ],lr\u001b[38;5;241m=\u001b[39mlr)\n\u001b[0;32m---> 11\u001b[0m _,val_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mopt\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdataset_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdataset_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43mclip\u001b[49m\u001b[43m,\u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43msave\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msize:\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, mul_factor:\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, layers:\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, agg:\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, dropout:\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, MSE:\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m     13\u001b[0m       \u001b[38;5;28mformat\u001b[39m(embed_size,multiple_factor,layers,agg,dropout,val_loss))\n",
      "File \u001b[0;32m~/Desktop/LC/wordle/functions_pt.py:126\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(opt, model, epochs, train_dl, val_dl, paras, clip, verbose, save)\u001b[0m\n\u001b[1;32m    124\u001b[0m data \u001b[38;5;241m=\u001b[39m [i\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m data]\n\u001b[1;32m    125\u001b[0m loss \u001b[38;5;241m=\u001b[39m model(data)\n\u001b[0;32m--> 126\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    127\u001b[0m clip_grad_value_(paras,clip)\n\u001b[1;32m    128\u001b[0m opt\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/_tensor.py:396\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    388\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    389\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    390\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    394\u001b[0m         create_graph\u001b[38;5;241m=\u001b[39mcreate_graph,\n\u001b[1;32m    395\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs)\n\u001b[0;32m--> 396\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/autograd/__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    168\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    170\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for _ in range(25):\n",
    "    embed_size = np.random.choice([64,128,160])\n",
    "    dropout = np.random.rand()*0.35\n",
    "    multiple_factor = np.random.choice([1,2])\n",
    "    layers = np.random.choice([2,3,4])\n",
    "    agg = np.random.choice(['mean','sum','max']) # mean,sum,min,max\n",
    "    model = pointNet(layers,embed_size,agg,dropout,multiple_factor).to('cuda')\n",
    "    opt = Adam([    {'params': [p for p in model.parameters() if p is not model.w]},\\\n",
    "                    {'params': model.w, 'lr': lr/4}\n",
    "                ],lr=lr)\n",
    "    _,val_loss = train(opt,model,epochs,dataset_train,dataset_val,model.parameters(),clip,verbose=False,save=False)\n",
    "    print('size:{}, mul_factor:{}, layers:{}, agg:{}, dropout:{}, MSE:{}\\n'.\n",
    "          format(embed_size,multiple_factor,layers,agg,dropout,val_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2b507a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'baseline0.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "296db821",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8b3b5512",
   "metadata": {},
   "outputs": [],
   "source": [
    "w = dataset_val.dataset.words_embed\n",
    "l = torch.tensor([2309.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8c5fdb68",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_40386/3274103529.py:1: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  model((torch.tensor(w).to('cuda'),torch.tensor(l).to('cuda')))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([2.8766], device='cuda:0', grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model((torch.tensor(w).to('cuda'),torch.tensor(l).to('cuda')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4762b0c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
